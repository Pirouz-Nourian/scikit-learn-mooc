{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "adult_census = pd.read_csv(\"../datasets/adult-census.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "target = adult_census[target_name]\n",
    "data = adult_census.drop(columns=[target_name, \"education-num\"])\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "categorical_columns = categorical_columns_selector(data)\n",
    "data_categorical = data[categorical_columns]\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# solution\n",
    "model = make_pipeline(\n",
    "    OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1),\n",
    "    LogisticRegression(max_iter=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mcross_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'2*n_jobs'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0merror_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Evaluate metric(s) by cross-validation and also record fit/score times.\n",
      "\n",
      "Read more in the :ref:`User Guide <multimetric_cross_validation>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "estimator : estimator object implementing 'fit'\n",
      "    The object to use to fit the data.\n",
      "\n",
      "X : array-like of shape (n_samples, n_features)\n",
      "    The data to fit. Can be for example a list, or an array.\n",
      "\n",
      "y : array-like of shape (n_samples,) or (n_samples, n_outputs),             default=None\n",
      "    The target variable to try to predict in the case of\n",
      "    supervised learning.\n",
      "\n",
      "groups : array-like of shape (n_samples,), default=None\n",
      "    Group labels for the samples used while splitting the dataset into\n",
      "    train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "    instance (e.g., :class:`GroupKFold`).\n",
      "\n",
      "scoring : str, callable, list, tuple, or dict, default=None\n",
      "    Strategy to evaluate the performance of the cross-validated model on\n",
      "    the test set.\n",
      "\n",
      "    If `scoring` represents a single score, one can use:\n",
      "\n",
      "    - a single string (see :ref:`scoring_parameter`);\n",
      "    - a callable (see :ref:`scoring`) that returns a single value.\n",
      "\n",
      "    If `scoring` represents multiple scores, one can use:\n",
      "\n",
      "    - a list or tuple of unique strings;\n",
      "    - a callable returning a dictionary where the keys are the metric\n",
      "      names and the values are the metric scores;\n",
      "    - a dictionary with metric names as keys and callables a values.\n",
      "\n",
      "    See :ref:`multimetric_grid_search` for an example.\n",
      "\n",
      "cv : int, cross-validation generator or an iterable, default=None\n",
      "    Determines the cross-validation splitting strategy.\n",
      "    Possible inputs for cv are:\n",
      "\n",
      "    - None, to use the default 5-fold cross validation,\n",
      "    - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "    - :term:`CV splitter`,\n",
      "    - An iterable yielding (train, test) splits as arrays of indices.\n",
      "\n",
      "    For int/None inputs, if the estimator is a classifier and ``y`` is\n",
      "    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "    other cases, :class:`.Fold` is used. These splitters are instantiated\n",
      "    with `shuffle=False` so the splits will be the same across calls.\n",
      "\n",
      "    Refer :ref:`User Guide <cross_validation>` for the various\n",
      "    cross-validation strategies that can be used here.\n",
      "\n",
      "    .. versionchanged:: 0.22\n",
      "        ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "\n",
      "n_jobs : int, default=None\n",
      "    Number of jobs to run in parallel. Training the estimator and computing\n",
      "    the score are parallelized over the cross-validation splits.\n",
      "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "    for more details.\n",
      "\n",
      "verbose : int, default=0\n",
      "    The verbosity level.\n",
      "\n",
      "fit_params : dict, default=None\n",
      "    Parameters to pass to the fit method of the estimator.\n",
      "\n",
      "pre_dispatch : int or str, default='2*n_jobs'\n",
      "    Controls the number of jobs that get dispatched during parallel\n",
      "    execution. Reducing this number can be useful to avoid an\n",
      "    explosion of memory consumption when more jobs get dispatched\n",
      "    than CPUs can process. This parameter can be:\n",
      "\n",
      "        - None, in which case all the jobs are immediately\n",
      "          created and spawned. Use this for lightweight and\n",
      "          fast-running jobs, to avoid delays due to on-demand\n",
      "          spawning of the jobs\n",
      "\n",
      "        - An int, giving the exact number of total jobs that are\n",
      "          spawned\n",
      "\n",
      "        - A str, giving an expression as a function of n_jobs,\n",
      "          as in '2*n_jobs'\n",
      "\n",
      "return_train_score : bool, default=False\n",
      "    Whether to include train scores.\n",
      "    Computing training scores is used to get insights on how different\n",
      "    parameter settings impact the overfitting/underfitting trade-off.\n",
      "    However computing the scores on the training set can be computationally\n",
      "    expensive and is not strictly required to select the parameters that\n",
      "    yield the best generalization performance.\n",
      "\n",
      "    .. versionadded:: 0.19\n",
      "\n",
      "    .. versionchanged:: 0.21\n",
      "        Default value was changed from ``True`` to ``False``\n",
      "\n",
      "return_estimator : bool, default=False\n",
      "    Whether to return the estimators fitted on each split.\n",
      "\n",
      "    .. versionadded:: 0.20\n",
      "\n",
      "error_score : 'raise' or numeric, default=np.nan\n",
      "    Value to assign to the score if an error occurs in estimator fitting.\n",
      "    If set to 'raise', the error is raised.\n",
      "    If a numeric value is given, FitFailedWarning is raised.\n",
      "\n",
      "    .. versionadded:: 0.20\n",
      "\n",
      "Returns\n",
      "-------\n",
      "scores : dict of float arrays of shape (n_splits,)\n",
      "    Array of scores of the estimator for each run of the cross validation.\n",
      "\n",
      "    A dict of arrays containing the score/time arrays for each scorer is\n",
      "    returned. The possible keys for this ``dict`` are:\n",
      "\n",
      "        ``test_score``\n",
      "            The score array for test scores on each cv split.\n",
      "            Suffix ``_score`` in ``test_score`` changes to a specific\n",
      "            metric like ``test_r2`` or ``test_auc`` if there are\n",
      "            multiple scoring metrics in the scoring parameter.\n",
      "        ``train_score``\n",
      "            The score array for train scores on each cv split.\n",
      "            Suffix ``_score`` in ``train_score`` changes to a specific\n",
      "            metric like ``train_r2`` or ``train_auc`` if there are\n",
      "            multiple scoring metrics in the scoring parameter.\n",
      "            This is available only if ``return_train_score`` parameter\n",
      "            is ``True``.\n",
      "        ``fit_time``\n",
      "            The time for fitting the estimator on the train\n",
      "            set for each cv split.\n",
      "        ``score_time``\n",
      "            The time for scoring the estimator on the test set for each\n",
      "            cv split. (Note time for scoring on the train set is not\n",
      "            included even if ``return_train_score`` is set to ``True``\n",
      "        ``estimator``\n",
      "            The estimator objects for each cv split.\n",
      "            This is available only if ``return_estimator`` parameter\n",
      "            is set to ``True``.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn import datasets, linear_model\n",
      ">>> from sklearn.model_selection import cross_validate\n",
      ">>> from sklearn.metrics import make_scorer\n",
      ">>> from sklearn.metrics import confusion_matrix\n",
      ">>> from sklearn.svm import LinearSVC\n",
      ">>> diabetes = datasets.load_diabetes()\n",
      ">>> X = diabetes.data[:150]\n",
      ">>> y = diabetes.target[:150]\n",
      ">>> lasso = linear_model.Lasso()\n",
      "\n",
      "Single metric evaluation using ``cross_validate``\n",
      "\n",
      ">>> cv_results = cross_validate(lasso, X, y, cv=3)\n",
      ">>> sorted(cv_results.keys())\n",
      "['fit_time', 'score_time', 'test_score']\n",
      ">>> cv_results['test_score']\n",
      "array([0.33150734, 0.08022311, 0.03531764])\n",
      "\n",
      "Multiple metric evaluation using ``cross_validate``\n",
      "(please refer the ``scoring`` parameter doc for more information)\n",
      "\n",
      ">>> scores = cross_validate(lasso, X, y, cv=3,\n",
      "...                         scoring=('r2', 'neg_mean_squared_error'),\n",
      "...                         return_train_score=True)\n",
      ">>> print(scores['test_neg_mean_squared_error'])\n",
      "[-3635.5... -3573.3... -6114.7...]\n",
      ">>> print(scores['train_r2'])\n",
      "[0.28010158 0.39088426 0.22784852]\n",
      "\n",
      "See Also\n",
      "---------\n",
      "cross_val_score : Run cross-validation for single metric evaluation.\n",
      "\n",
      "cross_val_predict : Get predictions from each split of cross-validation for\n",
      "    diagnostic purposes.\n",
      "\n",
      "sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n",
      "    loss function.\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\pirouz\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\n",
      "\u001b[1;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cv=cross_validate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.41128135, 0.33052993, 0.46844554, 0.42288828, 0.38499165]),\n",
       " 'score_time': array([0.03606796, 0.035923  , 0.03422976, 0.03577662, 0.0379405 ]),\n",
       " 'test_score': array([0.75514382, 0.75555328, 0.75573301, 0.75307125, 0.75788288])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=cross_validate(model,data_categorical,target,cv=5)\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('onehotencoder', OneHotEncoder(handle_unknown='ignore')),\n",
       "                ('logisticregression', LogisticRegression(max_iter=1000))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "model_hot=make_pipeline(OneHotEncoder(handle_unknown=\"ignore\"),LogisticRegression(max_iter=1000))\n",
    "model_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([1.00058603, 0.945786  , 0.90884542, 0.95601678, 1.03330612]),\n",
       " 'score_time': array([0.03381467, 0.03015566, 0.03296709, 0.03898692, 0.03249693]),\n",
       " 'test_score': array([0.83222438, 0.83560242, 0.82872645, 0.83312858, 0.83466421])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv=cross_validate(model_hot,data_categorical,target,cv=5)\n",
    "cv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8da4b0e5c9fd8631652fe5e3e82e7a986014bb02aabac28c50aff059d4ca1860"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
